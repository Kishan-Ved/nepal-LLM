{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://ne.wikipedia.org/w/index.php?title=%E0%A4%B5%E0%A4%BF%E0%A4%B6%E0%A5%87%E0%A4%B7:AllPages&from=%27e%27+%28%E0%A4%97%E0%A4%A3%E0%A4%BF%E0%A4%A4%E0%A5%80%E0%A4%AF+%E0%A4%85%E0%A4%9A%E0%A4%B0%29\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If chrome issue use the below code.\n",
    "try:\n",
    "    driver = webdriver.Chrome()\n",
    "except:\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    print(\"Running in headless mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"Title\", \"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    content = driver.find_elements(By.CLASS_NAME, \"mw-allpages-chunk\")\n",
    "    if len(content) < 1:\n",
    "        print(f\"Some problem with link: {base_url}\")\n",
    "    lists = content[0].find_elements(By.TAG_NAME, \"a\")\n",
    "    for li in lists:\n",
    "        title = li.get_attribute(\"title\")\n",
    "        url = li.get_attribute(\"href\")\n",
    "        df = pd.concat([df, pd.DataFrame({\"Title\": [title], \"URL\": [url]})], ignore_index=True)\n",
    "        i +=1\n",
    "    page_nav_classes = driver.find_elements(By.CLASS_NAME, \"mw-allpages-nav\")\n",
    "    link = page_nav_classes[-1].find_elements(By.TAG_NAME, \"a\")[-1].get_attribute(\"href\")\n",
    "    driver.get(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.to_csv(\"nepali_wikipedia_links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_links(language=\"ne\", limit=500):\n",
    "    \"\"\"\n",
    "    Function to get links of all articles from Wikipedia in a specific language.\n",
    "    By default, it fetches articles in Nepali (language='ne').\n",
    "    \"\"\"\n",
    "    S = requests.Session()\n",
    "\n",
    "    URL = f\"https://{language}.wikipedia.org/w/api.php\"\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"aplimit\": limit,  # Number of pages to return\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    article_links = []\n",
    "\n",
    "    while True:\n",
    "        response = S.get(url=URL, params=PARAMS)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract page titles and create links\n",
    "        pages = data['query']['allpages']\n",
    "        for page in pages:\n",
    "            title = page['title']\n",
    "            link = f\"https://{language}.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "            article_links.append(link)\n",
    "        \n",
    "        # Check if there's a 'continue' key to paginate through more results\n",
    "        if \"continue\" in data:\n",
    "            PARAMS.update(data['continue'])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return article_links\n",
    "\n",
    "# Fetch all Nepali Wikipedia links\n",
    "nepali_wiki_links = get_wikipedia_links(language=\"ne\")\n",
    "\n",
    "# Display first 10 links\n",
    "print(nepali_wiki_links[:10])\n",
    "\n",
    "# Optionally, save the links to a file\n",
    "with open('nepali_wikipedia_links.txt', 'w', encoding='utf-8') as f:\n",
    "    for link in nepali_wiki_links:\n",
    "        f.write(link + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Set up Chrome options for headless mode\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Create a folder for wiki articles if it doesn't exist\n",
    "if not os.path.exists('wiki_articles'):\n",
    "    os.mkdir('wiki_articles')\n",
    "\n",
    "# Read the links from 'nepali_wikipedia_links.txt'\n",
    "with open('nepali_wikipedia_links.txt', 'r', encoding='utf-8') as f:\n",
    "    links = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Limit to first 10 links for testing\n",
    "links = links[:10]\n",
    "\n",
    "# Initialize an index to keep track of file naming\n",
    "i = 0\n",
    "\n",
    "# Use tqdm for progress tracking\n",
    "for link in tqdm(links, desc=\"Scraping Wikipedia Links\"):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # Extract the heading (title of the page)\n",
    "        heading = driver.find_elements(By.TAG_NAME, 'h1')\n",
    "        if len(heading) < 1:\n",
    "            with open('wiki_articles_alerts.txt', 'a', encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Some problem with link (missing heading): {link}\\n\")\n",
    "            continue\n",
    "\n",
    "        # Use 'i' as the filename instead of the heading\n",
    "        filename = f'{i}.txt'\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if not os.path.exists(f'wiki_articles/{filename}'):\n",
    "            # Try extracting content from 'mw-body-content'\n",
    "            content = driver.find_elements(By.CLASS_NAME, 'mw-body-content')\n",
    "            \n",
    "            if len(content) < 1:\n",
    "                # Fallback: Try extracting from 'mw-parser-output' if 'mw-body-content' is not found\n",
    "                content = driver.find_elements(By.CLASS_NAME, 'mw-parser-output')\n",
    "                if len(content) < 1:\n",
    "                    with open('wiki_articles_alerts.txt', 'a', encoding=\"utf-8\") as f:\n",
    "                        f.write(f\"Some problem with link (missing content): {link}\\n\")\n",
    "                    continue\n",
    "\n",
    "            # Accumulate paragraph text, list items, and tables\n",
    "            data_str = ''\n",
    "\n",
    "            # Extract paragraphs\n",
    "            paras = content[0].find_elements(By.TAG_NAME, 'p')\n",
    "            for para in paras:\n",
    "                data_str += para.text + '\\n'\n",
    "\n",
    "            # Extract lists\n",
    "            lists = content[0].find_elements(By.TAG_NAME, 'li')\n",
    "            for li in lists:\n",
    "                if li.text:\n",
    "                    data_str += '- ' + li.text + '\\n'\n",
    "\n",
    "            # Extract infobox content only if it hasn't been captured yet\n",
    "            infobox = content[0].find_elements(By.CLASS_NAME, 'infobox')\n",
    "            if infobox:\n",
    "                infobox_text = '\\n'.join([box.text for box in infobox]) + '\\n'\n",
    "                \n",
    "                # If infobox text is not already in data_str, append it\n",
    "                if infobox_text not in data_str:\n",
    "                    data_str += infobox_text\n",
    "            nepali_content = re.sub(r'[a-zA-Z]', '', data_str)\n",
    "\n",
    "            # Save the content to a text file with the index number as filename\n",
    "            with open(f'wiki_articles/{filename}', 'w', encoding=\"utf-8\") as g:\n",
    "                g.write(f'Link: {link}\\n\\n' + nepali_content)\n",
    "\n",
    "        # Log successful saves to the alerts file\n",
    "        with open('wiki_articles_alerts.txt', 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Successfully saved: {filename}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with open('wiki_articles_alerts.txt', 'a', encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Error processing link: {link}, Error: {e}\\n\")\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
